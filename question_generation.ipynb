{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#NLP\n",
    "%pip install -U transformers==3.0.0\n",
    "%python -m nltk.downloader punkt\n",
    "%pip install pdfplumber\n",
    "%pip install pyyaml --upgrade\n",
    "%pip install transformers -U\n",
    "%pip install PyPDF2\n",
    "%pip install pdfplumber -q\n",
    "%pip install pipelines\n",
    "%pip install sentencepiece\n",
    "%pip install nltk\n",
    "%pip install protobuf\n",
    "%git clone https://github.com/patil-suraj/question_generation.git\n",
    "%pip install cv2\"\"\"\n",
    "#OCR\n",
    "print(0)\n",
    "import cv2\n",
    "print(1)\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "print(2)\n",
    "import pdfplumber\n",
    "import json\n",
    "print(3)\n",
    "import pdfplumber\n",
    "print(4)\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from question_generation import pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline as pepe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#%pip install --upgrade transformers huggingface-hub\n",
    "nlp = pipelines.pipeline('question-generation')\n",
    "nlp = pipelines.pipeline('question-generation', model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "nlp2 = pipelines.pipeline(\"multitask-qa-qg\")\n",
    "nlp2 = pipelines.pipeline(\"multitask-qa-qg\", model=\"valhalla/t5-base-qa-qg-hl\")\n",
    "\n",
    "nlp3 = pipelines.pipeline(\"e2e-qg\")\n",
    "nlp3 = pipelines.pipeline(\"e2e-qg\", model=\"valhalla/t5-base-e2e-qg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Automobili Lamborghini, the illustrious Italian manufacturer of luxury sports cars and SUVs, is headquartered in the picturesque Sant'Agata Bolognese. This renowned automotive institution boasts a storied legacy, and its contemporary success is firmly underpinned by a fascinating history that has seen it evolve through ownership changes, economic downturns, and groundbreaking innovations.\\\n",
    "Ferruccio Lamborghini, a prominent Italian industrialist with a passion for automobiles, laid the foundation for this iconic marque in 1963. His vision was audacious - to challenge the supremacy of Ferrari, the undisputed titan of Italian sports cars. Under Ferruccio's guidance, Automobili Ferruccio Lamborghini S.p.A. was established, and it immediately began making waves in the automotive world.\\\n",
    "One of the hallmarks of Lamborghini's early years was its distinctive rear mid-engine, rear-wheel-drive layout. This design philosophy became synonymous with Lamborghini's commitment to creating high-performance vehicles. The company's inaugural models, such as the 350 GT, arrived in the mid-1960s and showcased Lamborghini's dedication to precision engineering and uncompromising quality.\\\n",
    "Lamborghini's ascendancy was nothing short of meteoric during its formative decade. It consistently pushed the boundaries of automotive technology and design. However, the heady days of growth were met with a sudden downturn when the world faced the harsh realities of the 1973 global financial crisis and the subsequent oil embargo. Lamborghini, like many other automakers, grappled with plummeting sales and financial instability.\\\n",
    "Ownership of Lamborghini underwent multiple transitions in the wake of these challenges. The company faced bankruptcy in 1978, marking a turbulent chapter in its history. The ownership baton changed hands several times, with different entities attempting to steer the storied brand to calmer waters.\\\n",
    "In 1987, American automaker Chrysler Corporation took the helm at Lamborghini. The Chrysler era saw Lamborghini continue to produce remarkable vehicles like the Diablo while operating under the umbrella of a global conglomerate. However, it was not a permanent arrangement.\\\n",
    "In 1994, Malaysian investment group Mycom Setdco and Indonesian group V'Power Corporation acquired Lamborghini, signaling another phase of transformation for the company. These new custodians brought fresh perspectives and investment to the brand, fueling its resurgence.\\\n",
    "A significant turning point occurred in 1998 when Mycom Setdco and V'Power sold Lamborghini to the Volkswagen Group, which placed the Italian marque under the stewardship of its Audi division. This move brought newfound stability and resources, ensuring Lamborghini's enduring presence in the luxury sports car arena.\\\n",
    "Over the ensuing years, Lamborghini witnessed remarkable expansions in its product portfolio. The V10-powered Huracán captured the hearts of sports car enthusiasts with its exquisite design and formidable performance. Simultaneously, Lamborghini ventured into the SUV market with the Urus, a groundbreaking vehicle powered by a potent twin-turbo V8 engine. This diversification allowed Lamborghini to cater to a broader range of customers without compromising on its commitment to luxury and performance.\\\n",
    "While these successes were noteworthy, Lamborghini was not immune to the challenges posed by global economic fluctuations. In the late 2000s, during the worldwide financial crisis and the subsequent economic downturn, Lamborghini's sales experienced a significant decline, illustrating the brand's vulnerability to external economic factors.\\\n",
    "Despite these challenges, Lamborghini maintained its relentless pursuit of automotive excellence. The company's flagship model, the V12-powered Aventador, reached the pinnacle of automotive engineering and design before concluding its production run in 2022. However, the story does not end here. Lamborghini is set to introduce the Revuelto, a V12/electric hybrid model, in 2024, exemplifying its commitment to embracing cutting-edge technologies and pushing the boundaries of performance.\\\n",
    "In addition to its road car production, Lamborghini has made notable contributions to other industries. The company manufactures potent V12 engines for offshore powerboat racing, further underscoring its prowess in high-performance engineering.\\\n",
    "Interestingly, Lamborghini's legacy extends beyond the realm of automobiles. Ferruccio Lamborghini founded Lamborghini Trattori in 1948, a separate entity from the automobile manufacturer, which continues to produce tractors to this day.\\\n",
    "Lamborghini's rich history is also intertwined with the world of motorsport. In a stark contrast to his rival Enzo Ferrari, Ferruccio Lamborghini decided early on not to engage in factory-supported racing, considering it too expensive and resource-intensive. Nonetheless, Lamborghini's engineers, many of whom were passionate about racing, embarked on ambitious projects, including the development of the iconic Miura sports coupe, which possessed racing potential while being road-friendly. This project marked a pivotal moment in Lamborghini's history, showcasing its ability to create vehicles that could excel on both the track and the road.Despite Ferruccio's reluctance, Lamborghini did make some forays into motorsport. In the mid-1970s, while under the management of Georges-Henri Rossetti, Lamborghini collaborated with BMW to develop and manufacture 400 cars for BMW, a venture intended to meet Group 4 homologation requirements. However, due to financial instability and delays in development, BMW eventually took control of the project, finishing it without Lamborghini's involvement.\\\n",
    "Lamborghini also briefly supplied engines to Formula One teams from 1989 to 1993. Teams like Larrousse, Lotus, Ligier, Minardi, and Modena utilized Lamborghini power units during this period. Lamborghini's best result in Formula One was achieved when Aguri Suzuki finished third at the 1990 Japanese Grand Prix.\\\n",
    "In addition to Formula One, Lamborghini was involved in other racing series. Notably, racing versions of the Diablo were developed for the Diablo Supertrophy, a single-model racing series that ran from 1996 to 1999. The Murciélago R-GT, a production racing car, was created to compete in events like the FIA GT Championship and the American Le Mans Series in 2004, achieving notable results in its racing endeavors.\\\n",
    "Lamborghini's connection with motorsport reflects the brand's commitment to engineering excellence, even though it shied away from factory-backed racing for much of its history.\\\n",
    "Beyond the realms of automotive engineering, Lamborghini has carved a distinct niche in the world of branding. The company licenses its prestigious brand to manufacturers who produce a wide array of Lamborghini-branded consumer goods, including scale models, clothing, accessories, bags, electronics, and even laptop computers. This strategic approach has enabled Lamborghini to extend its brand reach beyond the confines of the automotive industry.\\\n",
    "One fascinating aspect of Lamborghini's identity is its deep connection with the world of bullfighting. In 1962, Ferruccio Lamborghini visited the ranch of Don Eduardo Miura, a renowned breeder of Spanish fighting bulls. Impressed by the majestic Miura animals, Ferruccio decided to adopt a raging bull as the emblem for his burgeoning automaker. This emblem, now iconic, symbolizes Lamborghini's passion for performance, power, and the thrill of the chase.\\\n",
    "Lamborghini's vehicle nomenclature also reflects this bullfighting heritage, with many models bearing the names of famous fighting bulls or bull-related themes. The Miura, named after the Miura bulls, set the precedent, and subsequent models like the Murciélago, Gallardo, and Aventador continued this tradition.\\\n",
    "Furthermore, Lamborghini has enthusiastically embraced emerging automotive technologies, responding to environmental concerns and changing consumer preferences. The Sian, introduced as the company's first hybrid model, showcases Lamborghini's commitment to sustainable performance. With its innovative hybrid powertrain, the Sian combines electric propulsion with a naturally aspirated V12 engine to deliver breathtaking performance while minimizing emissions.\\\n",
    "Looking ahead, Lamborghini has ambitious plans to produce an all-electric vehicle, aligning with the broader industry trend towards electrification. While traditionalists may lament the absence of roaring V12 engines, Lamborghini recognizes the importance of evolving with the times, ensuring that future generations of enthusiasts can experience the thrill of a Lamborghini while contributing to a more sustainable future.\\\n",
    "In summary, Automobili Lamborghini stands as a testament to the enduring allure of Italian craftsmanship and automotive\"\n",
    "\n",
    "text2 = \"Gravity (from Latin gravitas, meaning 'weight'), or gravitation, is a natural phenomenon by which all things with mass or energy—including planets, stars, galaxies, and even light—are brought toward (or gravitate toward) \\\n",
    "one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing \\\n",
    "and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly \\\n",
    "weaker as objects get further away\"\n",
    "\n",
    "text3 = \"42 is the answer to life, universe and everything.\"\n",
    "\n",
    "text4 = \"Forrest Gump is a 1994 American comedy-drama film directed by Robert Zemeckis and written by Eric Roth. \\\n",
    "It is based on the 1986 novel of the same name by Winston Groom and stars Tom Hanks, Robin Wright, Gary Sinise, \\\n",
    "Mykelti Williamson and Sally Field. The story depicts several decades in the life of Forrest Gump (Hanks), \\\n",
    "a slow-witted but kind-hearted man from Alabama who witnesses and unwittingly influences several defining \\\n",
    "historical events in the 20th century United States. The film differs substantially from the novel.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'question_generation'\n",
      "c:\\Users\\Kevin\\Desktop\\ace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where is Automobili Lamborghini headquartered?',\n",
       " 'Who laid the foundation for the iconic marque in 1963?',\n",
       " 'What was the hallmark of the 350 GT?',\n",
       " 'When did the company face bankruptcy?',\n",
       " 'In what year did Chrysler Corporation take the helm of the company?']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp3(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the Latin word for gravitation?\n",
      "Answer: gravitas\n",
      "\n",
      "Question: What does gravity give to physical objects on Earth?\n",
      "Answer: weight\n",
      "\n",
      "Question: The Moon's gravity causes what?\n",
      "Answer: ocean tides\n",
      "\n",
      "Question: Gravity has an infinite range, but its effects become weaker as objects get further away?\n",
      "Answer: Gravity\n",
      "\n",
      "Additional Question: What are the main features of the topic?\n",
      "Additional Answer: planets, stars, galaxies, and even light\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the different question answering model from Hugging Face Transformers\n",
    "\n",
    "\n",
    "# Your function that generates questions\n",
    "questions = nlp3(text2)\n",
    "\n",
    "# List to store answers corresponding to questions\n",
    "answers = []\n",
    "\n",
    "# Loop through each generated question\n",
    "for question in questions:\n",
    "    # Use the QA model to get an answer for each question\n",
    "    result = qa_pipeline(question=question, context=text2)\n",
    "    answer = result['answer']\n",
    "    answers.append(answer)  # Store the answers\n",
    "\n",
    "# Display questions and corresponding answers\n",
    "for i in range(len(questions)):\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])\n",
    "    print()\n",
    "\n",
    "# Generating another question using the same pipeline\n",
    "additional_question = \"What are the main features of the topic?\"\n",
    "additional_result = qa_pipeline(question=additional_question, context=text2)\n",
    "print(\"Additional Question:\", additional_question)\n",
    "print(\"Additional Answer:\", additional_result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who created Python?\n",
      "Answer: Guido van Rossum\n",
      "\n",
      "Question: When was Python released?\n",
      "Answer: 1991\n",
      "\n",
      "Question: What is Python's design philosophy?\n",
      "Answer: emphasizes code readability\n",
      "\n",
      "Additional Question: What are the main features of the topic?\n",
      "Additional Answer: code readability with its notable use of significant whitespace\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Your function that generates questions\n",
    "questions = nlp3(text)\n",
    "\n",
    "# List to store answers corresponding to questions\n",
    "answers = []\n",
    "\n",
    "# Loop through each generated question\n",
    "for question in questions:\n",
    "    # Use the QA model to get an answer for each question\n",
    "    result = qa_pipeline(question=question, context=text)\n",
    "    answer = result['answer']\n",
    "    answers.append(answer)  # Store the answers\n",
    "\n",
    "# Display questions and corresponding answers\n",
    "for i in range(len(questions)):\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])\n",
    "    print()\n",
    "\n",
    "# Generating another question using the same pipeline\n",
    "additional_question = \"What are the main features of the topic?\"\n",
    "additional_result = qa_pipeline(question=additional_question, context=text)\n",
    "print(\"Additional Question:\", additional_question)\n",
    "print(\"Additional Answer:\", additional_result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the answer to life, universe and everything?\n",
      "Answer: 42\n",
      "\n",
      "Additional Question: What are the main features of the topic?\n",
      "Additional Answer: life, universe and everything\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Your function that generates questions\n",
    "questions = nlp3(text3)\n",
    "\n",
    "# List to store answers corresponding to questions\n",
    "answers = []\n",
    "\n",
    "# Loop through each generated question\n",
    "for question in questions:\n",
    "    # Use the QA model to get an answer for each question\n",
    "    result = qa_pipeline(question=question, context=text3)\n",
    "    answer = result['answer']\n",
    "    answers.append(answer)  # Store the answers\n",
    "\n",
    "# Display questions and corresponding answers\n",
    "for i in range(len(questions)):\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])\n",
    "    print()\n",
    "\n",
    "# Generating another question using the same pipeline\n",
    "additional_question = \"What are the main features of the topic?\"\n",
    "additional_result = qa_pipeline(question=additional_question, context=text3)\n",
    "print(\"Additional Question:\", additional_question)\n",
    "print(\"Additional Answer:\", additional_result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who directed Forrest Gump?\n",
      "Answer: Robert Zemeckis\n",
      "\n",
      "Question: Who wrote the book of the same name?\n",
      "Answer: Winston Groom\n",
      "\n",
      "Question: What is the name of the 1994 American comedy-drama film?\n",
      "Answer: Forrest Gump\n",
      "\n",
      "Question: Which actor stars in the movie?\n",
      "Answer: Tom Hanks\n",
      "\n",
      "Question: How does the film differ from the novel?\n",
      "Answer: substantially\n",
      "\n",
      "Additional Question: What are the main features of the topic?\n",
      "Additional Answer: historical events in the 20th century United States\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Your function that generates questions\n",
    "questions = nlp3(text4)\n",
    "\n",
    "# List to store answers corresponding to questions\n",
    "answers = []\n",
    "\n",
    "# Loop through each generated question\n",
    "for question in questions:\n",
    "    # Use the QA model to get an answer for each question\n",
    "    result = qa_pipeline(question=question, context=text4)\n",
    "    answer = result['answer']\n",
    "    answers.append(answer)  # Store the answers\n",
    "\n",
    "# Display questions and corresponding answers\n",
    "for i in range(len(questions)):\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])\n",
    "    print()\n",
    "\n",
    "# Generating another question using the same pipeline\n",
    "additional_question = \"What are the main features of the topic?\"\n",
    "additional_result = qa_pipeline(question=additional_question, context=text4)\n",
    "print(\"Additional Question:\", additional_question)\n",
    "print(\"Additional Answer:\", additional_result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ParsedResults': [{'TextOverlay': {'Lines': [],\n",
       "    'HasOverlay': False,\n",
       "    'Message': 'Text overlay is not provided as it is not requested'},\n",
       "   'TextOrientation': '0',\n",
       "   'FileParseExitCode': 1,\n",
       "   'ParsedText': '\"Once upon a time a Tortoise and a Rabbit had an argument about who was\\r\\nfaster.\\r\\nThey decided to settle the argument with a race. They agreed on a route and\\r\\nstarted off the race. The rabbit shot ahead and ran briskly for some time. Then\\r\\nseeing that he was far ahead of the tortoise, he thought he\\'d sit under a tree\\r\\nfor some time and relax before continuing the race. He sat under the tree and\\r\\nsoon fell asleep. The tortoise plodding on overtook him and\\r\\nsoon finished the race, emerging as the undisputed champ. The rabbit woke\\r\\nup and realized that he\\'d lost the race.\"\\r\\n',\n",
       "   'ErrorMessage': '',\n",
       "   'ErrorDetails': ''}],\n",
       " 'OCRExitCode': 1,\n",
       " 'IsErroredOnProcessing': False,\n",
       " 'ProcessingTimeInMilliseconds': '1265',\n",
       " 'SearchablePDFURL': 'Searchable PDF not generated as it was not requested.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"image.png\")\n",
    "height, width, _ = img.shape\n",
    "\n",
    "roi = img\n",
    "\n",
    "url_api = \"https://api.ocr.space/parse/image\"\n",
    "_, compressedimage = cv2.imencode(\".png\", roi, [1, 90])\n",
    "file_bytes = io.BytesIO(compressedimage)\n",
    "\n",
    "result = requests.post(url_api,\n",
    "              files = {\"image.png\": file_bytes},\n",
    "              data = {\"apikey\": \"helloworld\",\n",
    "                      \"language\": \"eng\"})\n",
    "result = result.content.decode()\n",
    "result = json.loads(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did a Tortoise and a Rabbit have an argument about?\n",
      "Answer: who was\n",
      "faster\n",
      "\n",
      "Question: What did the tortoise decide to settle the argument with?\n",
      "Answer: a race\n",
      "\n",
      "Question: Who was the undisputed champ?\n",
      "Answer: The tortoise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Your OCR code\n",
    "img = cv2.imread(\"image.png\")\n",
    "height, width, _ = img.shape\n",
    "\n",
    "roi = img\n",
    "\n",
    "url_api = \"https://api.ocr.space/parse/image\"\n",
    "_, compressedimage = cv2.imencode(\".png\", roi, [1, 90])\n",
    "file_bytes = io.BytesIO(compressedimage)\n",
    "\n",
    "result_ocr = requests.post(url_api,\n",
    "                            files={\"image.png\": file_bytes},\n",
    "                            data={\"apikey\": \"helloworld\",\n",
    "                                  \"language\": \"eng\"})\n",
    "result_ocr = result_ocr.content.decode()\n",
    "result_ocr = json.loads(result_ocr)\n",
    "\n",
    "# Extract text from OCR result\n",
    "text_from_image = ' '.join([item['ParsedText'] for item in result_ocr['ParsedResults']])\n",
    "\n",
    "# Your function that generates questions\n",
    "questions = nlp3(text_from_image)\n",
    "\n",
    "# List to store answers corresponding to questions\n",
    "answers = []\n",
    "\n",
    "# Loop through each generated question\n",
    "for question in questions:\n",
    "    # Use the QA model to get an answer for each question\n",
    "    result_qa = qa_pipeline(question=question, context=text_from_image)\n",
    "    answer = result_qa['answer']\n",
    "    answers.append(answer)  # Store the answers\n",
    "\n",
    "# Display questions and corresponding answers\n",
    "for i in range(len(questions)):\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many little pigs once a time were there?\n",
      "Answer: three\n",
      "\n",
      "Question: What did one pig build his house out of?\n",
      "Answer: straw\n",
      "\n",
      "Question: How did the third pig work hard all day?\n",
      "Answer: built\n",
      "his house with bricks\n",
      "\n",
      "Question: Who saw the two little and ran and hid in their houses?\n",
      "Answer: big bad wolf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Your OCR code\n",
    "img = cv2.imread(\"image2.png\")\n",
    "height, width, _ = img.shape\n",
    "\n",
    "roi = img\n",
    "\n",
    "url_api = \"https://api.ocr.space/parse/image\"\n",
    "_, compressedimage = cv2.imencode(\".png\", roi, [1, 90])\n",
    "file_bytes = io.BytesIO(compressedimage)\n",
    "\n",
    "result_ocr = requests.post(url_api,\n",
    "                            files={\"image2.png\": file_bytes},\n",
    "                            data={\"apikey\": \"helloworld\",\n",
    "                                  \"language\": \"eng\"})\n",
    "result_ocr = result_ocr.content.decode()\n",
    "result_ocr = json.loads(result_ocr)\n",
    "\n",
    "# Extract text from OCR result\n",
    "text_from_image = ' '.join([item['ParsedText'] for item in result_ocr['ParsedResults']])\n",
    "\n",
    "# Your function that generates questions\n",
    "questions = nlp3(text_from_image)\n",
    "\n",
    "# List to store answers corresponding to questions\n",
    "answers = []\n",
    "\n",
    "# Loop through each generated question\n",
    "for question in questions:\n",
    "    # Use the QA model to get an answer for each question\n",
    "    result_qa = qa_pipeline(question=question, context=text_from_image)\n",
    "    answer = result_qa['answer']\n",
    "    answers.append(answer)  # Store the answers\n",
    "\n",
    "# Display questions and corresponding answers\n",
    "for i in range(len(questions)):\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: How many little pigs once a time were there?\n",
      "A. big bad wolf\n",
      "B. three\n",
      "C. three\n",
      "D. built\n",
      "his house with bricks\n",
      "Answer: B\n",
      "\n",
      "Question 2: What did one pig build his house out of?\n",
      "A. big bad wolf\n",
      "B. straw\n",
      "C. straw\n",
      "D. three\n",
      "Answer: B\n",
      "\n",
      "Question 3: How did the third pig work hard all day?\n",
      "A. built\n",
      "his house with bricks\n",
      "B. built\n",
      "his house with bricks\n",
      "C. three\n",
      "D. big bad wolf\n",
      "Answer: A\n",
      "\n",
      "Question 4: Who saw the two little and ran and hid in their houses?\n",
      "A. straw\n",
      "B. big bad wolf\n",
      "C. big bad wolf\n",
      "D. built\n",
      "his house with bricks\n",
      "Answer: B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the question answering model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# Your OCR code\n",
    "img = cv2.imread(\"image2.png\")\n",
    "height, width, _ = img.shape\n",
    "\n",
    "roi = img\n",
    "\n",
    "url_api = \"https://api.ocr.space/parse/image\"\n",
    "_, compressedimage = cv2.imencode(\".png\", roi, [1, 90])\n",
    "file_bytes = io.BytesIO(compressedimage)\n",
    "\n",
    "result_ocr = requests.post(url_api,\n",
    "                            files={\"image2.png\": file_bytes},\n",
    "                            data={\"apikey\": \"helloworld\",\n",
    "                                  \"language\": \"eng\"})\n",
    "result_ocr = result_ocr.content.decode()\n",
    "result_ocr = json.loads(result_ocr)\n",
    "\n",
    "# Extract text from OCR result\n",
    "text_from_image = ' '.join([item['ParsedText'] for item in result_ocr['ParsedResults']])\n",
    "\n",
    "# Your function that generates questions\n",
    "questions = nlp3(text_from_image)\n",
    "\n",
    "# List to store answers corresponding to questions\n",
    "answers = []\n",
    "\n",
    "# Loop through each generated question\n",
    "for question in questions:\n",
    "    # Use the QA model to get an answer for each question\n",
    "    result_qa = qa_pipeline(question=question, context=text_from_image)\n",
    "    answer = result_qa['answer']\n",
    "    answers.append(answer)  # Store the answers\n",
    "\n",
    "# Generate multiple-choice questions\n",
    "multiple_choice_questions = []\n",
    "for i, question in enumerate(questions):\n",
    "    distractors = random.sample(answers, k=3)\n",
    "    options = [answers[i]] + distractors\n",
    "    random.shuffle(options)\n",
    "    mcq = {\n",
    "        'question': question,\n",
    "        'options': options,\n",
    "        'answer': options.index(answers[i])  # Index of the correct answer\n",
    "    }\n",
    "    multiple_choice_questions.append(mcq)\n",
    "\n",
    "# Display multiple-choice questions and corresponding answers\n",
    "for i, mcq in enumerate(multiple_choice_questions):\n",
    "    print(f\"Question {i + 1}: {mcq['question']}\")\n",
    "    for j, option in enumerate(mcq['options']):\n",
    "        print(f\"{chr(65 + j)}. {option}\")\n",
    "    print(f\"Answer: {chr(65 + mcq['answer'])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.57k/4.57k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 5.67k/5.67k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 10.6MB/s]\n",
      "Downloading data: 100%|██████████| 25.4M/25.4M [00:46<00:00, 543kB/s] \n",
      "Generating test split: 100%|██████████| 4934/4934 [00:02<00:00, 2213.32 examples/s] \n",
      "Generating train split: 100%|██████████| 87866/87866 [00:07<00:00, 11116.89 examples/s]\n",
      "Generating validation split: 100%|██████████| 4887/4887 [00:02<00:00, 2227.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from typing import List, Dict\n",
    "import tqdm.notebook as tq\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    "    )\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"race\", 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\\nGiven that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\\n\"Surgery ,\" one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\\nOne girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\\nAt that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles, as I\\'m not trying to hide the fact that I am just not tall!\\nIt seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\\nNo one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'B', 'A', 'D']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(dataset['train']['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doctor', 'model', 'teacher', 'reporter']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['options'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kevin\\Desktop\\ace\\question_generation.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         data_rows\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m: curr_context,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m: curr_question,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mincorrect3\u001b[39m\u001b[39m'\u001b[39m: curr_incorrect3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         })\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(data_rows)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m race_train_df \u001b[39m=\u001b[39m create_dataset(dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\Kevin\\Desktop\\ace\\question_generation.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_dataset\u001b[39m(dataset_split):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     data_rows \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(dataset_split))):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         curr_context \u001b[39m=\u001b[39m dataset_split[i][\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Desktop/ace/question_generation.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         curr_question \u001b[39m=\u001b[39m dataset_split[i][\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tqdm\\notebook.py:233\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m    232\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[1;32m--> 233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[0;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from typing import List, Dict\n",
    "import tqdm.notebook as tq\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    "    )\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = 't5-small'\n",
    "LEARNING_RATE = 0.0001\n",
    "SOURCE_MAX_TOKEN_LEN = 512\n",
    "TARGET_MAX_TOKEN_LEN = 64\n",
    "SEP_TOKEN = '<sep>'\n",
    "TOKENIZER_LEN = 32101 #after adding the new <sep> token\n",
    "\n",
    "# Model\n",
    "class QGModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "        self.model.resize_token_embeddings(TOKENIZER_LEN) #resizing after adding new tokens to the tokenizer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "  \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "class DistractorGenerator():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "        # print('tokenizer len before: ', len(self.tokenizer))\n",
    "        self.tokenizer.add_tokens(SEP_TOKEN)\n",
    "        # print('tokenizer len after: ', len(self.tokenizer))\n",
    "        self.tokenizer_len = len(self.tokenizer)\n",
    "\n",
    "        checkpoint_path = 'app/ml_models/distractor_generation/models/race-distractors.ckpt'\n",
    "        self.dg_model = QGModel.load_from_checkpoint(checkpoint_path)\n",
    "        self.dg_model.freeze()\n",
    "        self.dg_model.eval()\n",
    "\n",
    "    def generate(self, generate_count: int, correct: str, question: str, context: str) -> List[str]:\n",
    "        \n",
    "        generate_triples_count = int(generate_count / 3) + 1 #since this model generates 3 distractors per generation\n",
    "        \n",
    "        model_output = self._model_predict(generate_triples_count, correct, question, context)\n",
    "\n",
    "        cleaned_result = model_output.replace('<pad>', '').replace('</s>', '<sep>')\n",
    "        cleaned_result = self._replace_all_extra_id(cleaned_result)\n",
    "        distractors = cleaned_result.split('<sep>')[:-1]\n",
    "        distractors = [x.translate(str.maketrans('', '', string.punctuation)) for x in distractors]\n",
    "        distractors = list(map(lambda x: x.strip(), distractors))\n",
    "\n",
    "        return distractors\n",
    "\n",
    "    def _model_predict(self, generate_count: int, correct: str, question: str, context: str) -> str:\n",
    "        source_encoding = self.tokenizer(\n",
    "            '{} {} {} {} {}'.format(correct, SEP_TOKEN, question, SEP_TOKEN, context),\n",
    "            max_length= SOURCE_MAX_TOKEN_LEN,\n",
    "            padding='max_length',\n",
    "            truncation= True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "            )\n",
    "\n",
    "        generated_ids = self.dg_model.model.generate(\n",
    "            input_ids=source_encoding['input_ids'],\n",
    "            attention_mask=source_encoding['attention_mask'],\n",
    "            num_beams=generate_count,\n",
    "            num_return_sequences=generate_count,\n",
    "            max_length=TARGET_MAX_TOKEN_LEN,\n",
    "            repetition_penalty=2.5,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        preds = {\n",
    "            self.tokenizer.decode(generated_id, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "            for generated_id in generated_ids\n",
    "        }\n",
    "\n",
    "        return ''.join(preds)\n",
    "\n",
    "    def _correct_index_of(self, text:str, substring: str, start_index: int = 0):\n",
    "        try:\n",
    "            index = text.index(substring, start_index)\n",
    "        except ValueError:\n",
    "            index = -1\n",
    "\n",
    "        return index\n",
    "\n",
    "    def _replace_all_extra_id(self, text: str):\n",
    "        new_text = text\n",
    "        start_index_of_extra_id = 0\n",
    "\n",
    "        while (self._correct_index_of(new_text, '<extra_id_') >= 0):\n",
    "            start_index_of_extra_id = self._correct_index_of(new_text, '<extra_id_', start_index_of_extra_id)\n",
    "            end_index_of_extra_id = self._correct_index_of(new_text, '>', start_index_of_extra_id)\n",
    "\n",
    "            new_text = new_text[:start_index_of_extra_id] + '<sep>' + new_text[end_index_of_extra_id + 1:]\n",
    "\n",
    "        return new_text\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
